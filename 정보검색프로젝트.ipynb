{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "정보검색프로젝트.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBtYg7S1r6if"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KncDgXSJKmZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.python.keras.backend import softmax\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from konlpy.tag import Okt\n",
        "import os\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UBbD16FHhiv"
      },
      "source": [
        "데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6LGqMzbJd1P"
      },
      "source": [
        "ilbe = pd.read_excel('/content/drive/MyDrive/Ilbe0.xlsx')\n",
        "\n",
        "clien= pd.read_excel('/content/drive/MyDrive/Clien0.xlsx')\n",
        "\n",
        "OU = pd.read_excel('/content/drive/MyDrive/OU3.xlsx')\n",
        "\n",
        "Moon = pd.read_excel('/content/drive/MyDrive/Moon.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxqrb4LzPbG1"
      },
      "source": [
        "right=ilbe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe5nIkNeI5Xc"
      },
      "source": [
        "left = pd.concat([clien, OU, Moon])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkCb1cBoPoT0"
      },
      "source": [
        "right['label']=1\n",
        "left['label']=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReSNoNoKPtoN"
      },
      "source": [
        "left = left[['title', 'label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZEOGny-Jo6W"
      },
      "source": [
        "right=right[['title','label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOznlDr9Kw-_"
      },
      "source": [
        "print(right.isnull().sum())\n",
        "print(left.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mszz5afRK_OE"
      },
      "source": [
        "right.dropna(axis=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mcnzh8P3QRsW"
      },
      "source": [
        "left.reset_index(drop=True, inplace=True)\n",
        "right.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjGot6DkNHZR"
      },
      "source": [
        "# 정규 표현식을 통한 한글 외 문자 제거\n",
        "for i in range(len(right)):\n",
        "    right['title'][i] = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", right['title'][i])\n",
        "for i in range(len(left)):\n",
        "    left['title'][i] = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", left['title'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXYJMeAoW7SK"
      },
      "source": [
        "print(left.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi_bacXOW9Q8"
      },
      "source": [
        "print(right.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjyEFOQQV2wb"
      },
      "source": [
        "data=pd.concat([right,left])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSYqREzdUqtC"
      },
      "source": [
        "# from hanspell import spell_checker\n",
        "# x=[]\n",
        "# for i in X:\n",
        "#     spelled_sent = spell_checker.check(i)\n",
        "#     hanspell_sent = spelled_sent.checked\n",
        "#     x.append(hanspell_sent)\n",
        "# 맞춤법 검사기가 사람이름을 교정해버려서 적용한것이 오류가 더 많음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-lV-MG4PlTp"
      },
      "source": [
        "data['title'].replace('', np.nan, inplace=True)\n",
        "print(data.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzPiZEzPLWTR"
      },
      "source": [
        "data.dropna(axis=0, inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLGBDkh4kbaC"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3r9kLbmn0p2"
      },
      "source": [
        "X=data['title']\n",
        "y=data['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPhSPzabgjJ"
      },
      "source": [
        "Okt 형태소 분석기에 단어 추가\n",
        "https://sirzzang.github.io/self/SELF-konlpy-userdic/   \n",
        "링크의 설명대로 진행하시면 됩니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRymvGS-br9N"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip3 install JPype1-py3\n",
        "\n",
        "# automake 설치\n",
        "os.chdir('/tmp')\n",
        "!curl -LO http://ftpmirror.gnu.org/automake/automake-1.11.tar.gz\n",
        "!tar -zxvf automake-1.11.tar.gz\n",
        "os.chdir('/tmp/automake-1.11')\n",
        "!./configure\n",
        "!make\n",
        "!make install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaQlj_3rcCy5"
      },
      "source": [
        "# automake 설치 오류 시\n",
        "os.chdir('/tmp/') \n",
        "!wget -O m4-1.4.9.tar.gz http://ftp.gnu.org/gnu/m4/m4-1.4.9.tar.gz\n",
        "!tar -zvxf m4-1.4.9.tar.gz\n",
        "os.chdir('/tmp/m4-1.4.9')\n",
        "!./configure\n",
        "!make\n",
        "!make install\n",
        "os.chdir('/tmp')\n",
        "!curl -OL http://ftpmirror.gnu.org/autoconf/autoconf-2.69.tar.gz\n",
        "!tar xzf autoconf-2.69.tar.gz\n",
        "os.chdir('/tmp/autoconf-2.69')\n",
        "!./configure --prefix=/usr/local\n",
        "!make\n",
        "!make install\n",
        "!export PATH=/usr/local/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ7Gj5LNdH-v"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/usr/local/lib/python3.6/dist-packages/konlpy/java')\n",
        "os.getcwd() \n",
        "os.makedirs('./aaa')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpRFm2wuzKs6"
      },
      "source": [
        "/usr/local/lib/python3.6/dist-packages/konlpy/java  안의 open-korean-text-2.1.0.jar 파일을   aaa 폴더로 옮겨줍니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psjyaMnhdT7T"
      },
      "source": [
        "os.chdir('/usr/local/lib/python3.6/dist-packages/konlpy/java/aaa')\n",
        "!jar xvf open-korean-text-2.1.0.jar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whLGHmK-bj7c"
      },
      "source": [
        "# 사용자 사전 열기\n",
        "with open(f\"/usr/local/lib/python3.6/dist-packages/konlpy/java/aaa/org/openkoreantext/processor/util/noun/names.txt\", \"r\") as f:\n",
        "    d = f.read()\n",
        "# 단어추가\n",
        "d += '문통\\n문재앙\\n문프\\n문대통령\\n국민의힘\\n박근혜\\n이명박근혜\\n미통당\\n미래통합당\\n미통닭\\n국민의힘\\n국힘당\\n국힘\\n국민의짐\\n더불당\\n더민주\\n더불어민주당\\n더듬어만진당\\n자한당\\n자유한국당\\n'\n",
        "\n",
        "# 파일 새롭게 저장\n",
        "# 사용자 사전 열기\n",
        "with open(f\"/usr/local/lib/python3.6/dist-packages/konlpy/java/aaa/org/openkoreantext/processor/util/noun/names.txt\", \"w\") as f:\n",
        "    f.write(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85htD7Febjxe"
      },
      "source": [
        "os.chdir('/usr/local/lib/python3.6/dist-packages/konlpy/java/aaa')\n",
        "!jar cvf ../open-korean-text-2.1.0.jar * "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6zshtl3x7lt"
      },
      "source": [
        "여기까지 한 뒤 런타임 다시시작 (초기화X)   \n",
        "jvm error가 뜬다면 jvm.py의 convertStrings=True 에 주석처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chAkLO47n9SZ"
      },
      "source": [
        "# 단어 추가된것 확인\n",
        "okt=Okt()\n",
        "okt.morphs('문재인의 별명은 문통, 문프, 문재앙이며 국민의힘이 아닌 민주당이다')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JhzsI_Q2L-u"
      },
      "source": [
        "X1 = []\n",
        "okt=Okt()\n",
        "stopterms = ['듯','의','가','을','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','하는','하기','기','등']\n",
        "for sentence in X:\n",
        "    temp_X = []\n",
        "    temp_X = okt.morphs(sentence) # 토큰화\n",
        "    temp_X = [word for word in temp_X if not word in stopterms] # 불용어 제거\n",
        "    print(temp_X)\n",
        "    X1.append(temp_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxunnbHqVjeg"
      },
      "source": [
        "pip install soylemma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGwhWgAuWWxg"
      },
      "source": [
        "https://ratsgo.github.io/korean%20linguistics/2017/03/16/words2/\n",
        "\n",
        "한국어 종결어미 참고"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWYqBqnXFb4G"
      },
      "source": [
        "from soylemma import Lemmatizer\n",
        "\n",
        "\n",
        "def stem_words(X):\n",
        "    stemmed=[]\n",
        "    lemmatizer = Lemmatizer()\n",
        "    for i in X:\n",
        "        temp = []\n",
        "        temp_stemmed = [word if lemmatizer.lemmatize(word)==[] else lemmatizer.lemmatize(word)[0][0] for word in i]\n",
        "        for i in range(len(temp_stemmed)):\n",
        "            if '겠' in temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('겠')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'다')\n",
        "                print(temp_stemmed[i])\n",
        "            if '냈' in temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('냈')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'내다')\n",
        "                print(temp_stemmed[i])\n",
        "            if '겼' in temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('겼')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'기다')\n",
        "                print(temp_stemmed[i])\n",
        "            if '녔' in temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('녔')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'니다')\n",
        "                print(temp_stemmed[i])\n",
        "            if '렸' in temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('렸')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'리다')\n",
        "                print(temp_stemmed[i])\n",
        "            if '졌' in temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('졌')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'다')\n",
        "                print(temp_stemmed[i])\n",
        "            if '했' in temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('했')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'하다')\n",
        "                print(temp_stemmed[i])\n",
        "            if '었' in temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('었')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'다')\n",
        "                print(temp_stemmed[i])\n",
        "            if '았' in temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('았')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'다')\n",
        "                print(temp_stemmed[i])\n",
        "            if '한다' in temp_stemmed[i] and not '한다' == temp_stemmed[i]: \n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('한다')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'')\n",
        "                print(temp_stemmed[i])\n",
        "            if '됐다' in temp_stemmed[i] and not '됐다' == temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('됐다')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'')\n",
        "                print(temp_stemmed[i])\n",
        "            if '는다' in temp_stemmed[i]:\n",
        "                print(temp_stemmed[i] + '->', end='')\n",
        "                a=temp_stemmed[i].find('는다')\n",
        "                temp_stemmed[i]=temp_stemmed[i].replace(temp_stemmed[i][a:],'다')     \n",
        "                print(temp_stemmed[i])\n",
        "        stemmed.append(temp_stemmed)\n",
        "    return stemmed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcH1vFXRaBPF"
      },
      "source": [
        "stemmed = stem_words(X1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17BH6_CpufBC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(stemmed, \n",
        "                                                    y, \n",
        "                                                    test_size=0.2,\n",
        "                                                    shuffle=True, \n",
        "                                                    random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLQvxvxEXcyh"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TahS5iq1GmyM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFFOCjbtY235"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euzX3qK0ZIdF"
      },
      "source": [
        "\n",
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "        print(key)\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEsx91ufeJ3S"
      },
      "source": [
        "vocab_size = total_cnt - rare_cnt + 2\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWw6KsmTWFAy"
      },
      "source": [
        "print(len(X_train), len(X_test), len(y_train), len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94SQm8RDeln7"
      },
      "source": [
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri2ddr1e4QDe"
      },
      "source": [
        "print('댓글 최대 길이 :',max(len(l) for l in X_train))\n",
        "print('댓글 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(s) for s in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOdxdbBf4SF4"
      },
      "source": [
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k26vFSt4age"
      },
      "source": [
        "max_len = 30\n",
        "below_threshold_len(max_len, X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOUKumnv4dV4"
      },
      "source": [
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCvn_ig63y-L"
      },
      "source": [
        "모델 구현 및 임베딩층"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OJ_KbUr4jW1"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, SimpleRNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMaDiAajSFSF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split( X_train,\n",
        "                                                    y_train, \n",
        "                                                    test_size=0.1,\n",
        "                                                    shuffle=True, \n",
        "                                                    random_state=11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erEJorxORZDR"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 32)) # 임베딩 벡터의 차원은 32\n",
        "model.add(SimpleRNN(32)) # RNN 셀의 hidden_size는 32\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSbovH8CRZSA"
      },
      "source": [
        "# 최적의 파라미터 저장\n",
        "mc = ModelCheckpoint('model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=100,\n",
        "          callbacks=[mc],\n",
        "          validation_data = (X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckGOn920RZWQ"
      },
      "source": [
        "def plot(history):\n",
        "    epochs = range(1, len(history.history['acc']) + 1)\n",
        "    plt.plot(epochs, history.history['acc'])\n",
        "    plt.plot(epochs, history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.legend(['train', 'val'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    epochs = range(1, len(history.history['loss']) + 1)\n",
        "    plt.plot(epochs, history.history['loss'])\n",
        "    plt.plot(epochs, history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.legend(['train', 'val'], loc='upper right')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sfcRh9qRZbB"
      },
      "source": [
        "plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0PX0tW0T2Uo"
      },
      "source": [
        "loaded_model = load_model('model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBxCfS0pUGop"
      },
      "source": [
        "def sentiment_predict(new_sentence, model):\n",
        "    new_sentence.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "    new_sentence = [word for word in new_sentence if not word in stopterms] # 불용어 제거\n",
        "    new_sentence=stem_words([new_sentence])[0]\n",
        "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "    score = float(model.predict(pad_new)) # 예측\n",
        "    if(score < 0.5):\n",
        "        print(\"{:.2f}% 확률로 좌파입니다.\\n\".format((1-score) * 100))\n",
        "    else:\n",
        "        print(\"{:.2f}% 확률로 우파입니다.\\n\".format(score * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpPNFSnOUHEN"
      },
      "source": [
        "sentiment_predict('저는 문재인 지지자 입니다', loaded_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBJoPOFDUHHy"
      },
      "source": [
        "sentiment_predict('부동산 폭등의 주범 문재인 물러나라!', loaded_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QK4pPUoUHMM"
      },
      "source": [
        "sentiment_predict('문 대통령의 위상은 날로 높아집니다', loaded_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swBMpAEcUfZT"
      },
      "source": [
        "sentiment_predict('문재앙을 탄핵하자', loaded_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbHgtsVRUf-3"
      },
      "source": [
        "sentiment_predict('문프 힘내세요!', loaded_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL74EUfaUgDn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2UQJHDkUgIa"
      },
      "source": [
        "import nltk\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import string\n",
        "\n",
        "model = Word2Vec(stemmed ,window = 5,min_count=2,sg=1,iter=1000)\n",
        "\n",
        "print(list(model.wv.vocab.keys()))\n",
        "print(\"vocab length : %d\"%len(model.wv.vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPQ-niAhWRel"
      },
      "source": [
        "model.wv.most_similar(\"문재인\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXIfZN5eW2Jz"
      },
      "source": [
        "def get_vector(word,model):\n",
        "    if word in model:\n",
        "        return model[word]\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NR-91XIZQR_"
      },
      "source": [
        "model.vector_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiKkyrgaZLkh"
      },
      "source": [
        "embedding_matrix = np.zeros((17976, 100))\n",
        "# 단어 집합 크기의 행과 200개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_KDOllKWR6t"
      },
      "source": [
        "for word, i in tokenizer.word_index.items(): # 훈련 데이터의 단어 집합에서 단어와 정수 인덱스를 1개씩 꺼내온다.\n",
        "    temp = get_vector(word,model) # 단어(key) 해당되는 임베딩 벡터의 300개의 값(value)를 임시 변수에 저장\n",
        "    if temp is not None: # 만약 None이 아니라면 임베딩 벡터의 값을 리턴받은 것이므로\n",
        "        embedding_matrix[i] = temp # 해당 단어 위치의 행에 벡터의 값을 저장한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTxit7iaWR_Z"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "model = Sequential()\n",
        "e = Embedding(17976, 100, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT3r7JUqWSEQ"
      },
      "source": [
        "# 최적의 파라미터 저장\n",
        "mc = ModelCheckpoint('model.h6', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=100,\n",
        "          callbacks=[mc],\n",
        "          validation_data = (X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUEQvlgqb0TC"
      },
      "source": [
        "plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOr0iXkicCo7"
      },
      "source": [
        "loaded_model2 = load_model('model.h6')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model2.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I3P5k62cI4S"
      },
      "source": [
        "sentiment_predict('저는 문재인 지지자 입니다', loaded_model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGFP-cricJNn"
      },
      "source": [
        "sentiment_predict('부동산 폭등의 주범 문재인 물러나라!', loaded_model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAYJvbGIcfx9"
      },
      "source": [
        "sentiment_predict('문 대통령의 위상은 날로 높아집니다', loaded_model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTISz3wHb0a7"
      },
      "source": [
        "sentiment_predict('문재앙을 탄핵하자', loaded_model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VovFgGDcuKH"
      },
      "source": [
        "sentiment_predict('문프 힘내세요!', loaded_model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQd6CkKANJIz"
      },
      "source": [
        "from gensim import models\n",
        "from gensim.models import FastText\n",
        "#Fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCfA-j0mPqX_"
      },
      "source": [
        "F_model = FastText(stemmed, size=200, workers=4, sg=1, iter=6, word_ngrams=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7YhsiQ6xRXF"
      },
      "source": [
        "len(X_train),len(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws6UPdiP27ES"
      },
      "source": [
        "F_model.vector_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D8lRbEycyhn"
      },
      "source": [
        "for word, i in tokenizer.word_index.items(): # 훈련 데이터의 단어 집합에서 단어와 정수 인덱스를 1개씩 꺼내온다.\n",
        "    temp = get_vector(word,F_model) # 단어(key) 해당되는 임베딩 벡터의 300개의 값(value)를 임시 변수에 저장\n",
        "    if temp is not None: # 만약 None이 아니라면 임베딩 벡터의 값을 리턴받은 것이므로\n",
        "        embedding_matrix[i] = temp # 해당 단어 위치의 행에 벡터의 값을 저장한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV7eIdRrp4_w"
      },
      "source": [
        "F_model = Sequential()\n",
        "e = Embedding(17976, 200, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
        "F_model.add(e)\n",
        "F_model.add(Flatten())\n",
        "F_model.add(Dense(1, activation='sigmoid'))\n",
        "F_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "F_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVg2C7YrOdDQ"
      },
      "source": [
        "# 최적의 파라미터 저장\n",
        "\n",
        "mc = ModelCheckpoint('F_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ieOT73N4LJs"
      },
      "source": [
        "history = F_model.fit(X_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=100,\n",
        "          callbacks=[mc],\n",
        "          validation_data = (X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnoo5VaJ4r68"
      },
      "source": [
        "loaded_F_model = load_model('F_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_F_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m72ii4re78FP"
      },
      "source": [
        "sentiment_predict('저는 문재인 지지자 입니다', loaded_F_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCZELVpzB51M"
      },
      "source": [
        "sentiment_predict('부동산 폭등의 주범 문재인 물러나라!', loaded_F_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtoHOYHFGrgM"
      },
      "source": [
        "sentiment_predict('문 대통령의 위상은 날로 높아집니다', loaded_F_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P6TL6duG8Dx"
      },
      "source": [
        "sentiment_predict('문재앙을 탄핵하자', loaded_F_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY9c__2JHAtc"
      },
      "source": [
        "sentiment_predict('문프 힘내세요!', loaded_F_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvwFoLjsHCe1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}